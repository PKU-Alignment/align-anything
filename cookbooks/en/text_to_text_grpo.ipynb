{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Models with GRPO and RLVR Algorithms\n",
    "\n",
    "This tutorial demonstrates how to fine-tune large language models (using **Llama-3.1-8B-Instruct** as an example) using the **Group Relative Policy Optimization (GRPO)** algorithm. Through this tutorial, you will learn how to customize reward functions for your tasks under the **Align Anything** framework, and combine them with **Reinforcement Learning with Verifiable Rewards (RLVR)** to further improve model performance on specific tasks.\n",
    "\n",
    "## 1.1 What is GRPO?\n",
    "\n",
    "**Group Relative Policy Optimization (GRPO)** is a reinforcement learning algorithm designed to enhance model reasoning capabilities through grouping and relative reward mechanisms. GRPO was first introduced in the paper *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models* and was successfully applied in the post-training phase of DeepSeek-R1.\n",
    "\n",
    "GRPO aims to optimize model behavior through relative comparison policies rather than absolute rewards. Specifically, GRPO groups multiple model outputs and calculates reward values based on their relative performance. This approach helps mitigate issues in traditional reinforcement learning where absolute rewards are difficult to define or lack precision, making it particularly suitable for complex reasoning tasks.\n",
    "\n",
    "## 1.2 What is RLVR?\n",
    "\n",
    "**Reinforcement Learning with Verifiable Rewards (RLVR)** is a novel language model training method designed for tasks with verifiable outcomes (such as mathematical problem-solving and instruction following). RLVR uses existing reinforcement learning reward mechanisms (like RLHF) but replaces traditional reward models with a verification function.\n",
    "\n",
    "Unlike traditional methods, RLVR trains models using binary signals through answer matching or constraint verification (e.g., whether an answer is correct). When applied to mathematical domains or other verifiable tasks, RLVR not only improves performance on specific benchmarks (like GSM8K) but also maintains stable performance across other tasks.\n",
    "\n",
    "RLVR can be viewed as a simplified version of existing methods, such as RL with execution feedback or bootstrapping methods for language model reasoning. Its core idea is to use verifiable signals as direct rewards, avoiding the complex process of building sophisticated reward models.\n",
    "\n",
    "## 2. Environment Setup\n",
    "\n",
    "Before starting, please make sure you have installed the ``align-anything`` package.\n",
    "\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone git@github.com:PKU-Alignment/align-anything.git\n",
    "cd align-anything\n",
    "\n",
    "# Create a virtual environment using conda\n",
    "conda create -n align-anything python==3.11\n",
    "conda activate align-anything\n",
    "```\n",
    "\n",
    "- **`[Optional]`** We recommend installing [CUDA](https://anaconda.org/nvidia/cuda) in the conda environment and set the environment variable.\n",
    "\n",
    "```bash\n",
    "# We have tested this version of CUDA on the H800 computing cluster and it worked well.\n",
    "# You can adjust this version according to your actual computing cluster.\n",
    "\n",
    "conda install nvidia/label/cuda-12.2.0::cuda\n",
    "export CUDA_HOME=$CONDA_PREFIX\n",
    "```\n",
    "\n",
    "> If your CUDA is installed in a different location, such as `/usr/local/cuda/bin/nvcc`, you can set the environment variable as follows:\n",
    "\n",
    "```bash\n",
    "export CUDA_HOME=\"/usr/local/cuda\"\n",
    "```\n",
    "\n",
    "Finally, install `align-anything` using the following command:\n",
    "\n",
    "```bash\n",
    "# We have prepared a quick installation for training and evaluation.\n",
    "# If you only need to use the training or evaluation module,\n",
    "# you can install the corresponding dependencies.\n",
    "pip install -e .[train] # Install training dependencies\n",
    "pip install -e .[evaluate] # Install evaluation dependencies\n",
    "\n",
    "# If you need to install all dependencies, you can use the following command:\n",
    "pip install -e .[all]\n",
    "```\n",
    "\n",
    "At last, according to https://github.com/PKU-Alignment/align-anything/tree/main/align_anything/models/remote_rm\n",
    "\n",
    "You should \n",
    "```bash\n",
    "pip install Levenshtein flask latex2sympy2_extended math_verify\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Llama-3.1-8B-Instruct Model Output Example\n",
    "Next, let's first test the zero-shot capability of the Llama-3.1-8B-Instruct model.\n",
    "\n",
    "### 3.1 Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1742778596.498488] [dsw-519274-66f65ff576-678dh:4051137:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the Original Llama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"  # Set device to \"cuda\" to use GPU\n",
    "model_path = (\n",
    "    \"/PATH/TO/YOUR/Meta-Llama-3.1-8B-Instruct\"  # Please replace with your actual model path\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test the Performance of the Original Model\n",
    "\n",
    "Let's test the Llama-3.1-8B-Instruct model with a sample question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text: The sequence of square roots of the positive integers is increasing. The largest term of the sequence that is less than or equal to 20 is $\\sqrt{19}$, the square root of 16. Therefore, 16 terms of the sequence are less than or equal to 20. The sequence of 16 terms is\n",
      "\n",
      "$\\sqrt{1},\\sqrt{2},\\sqrt{3},\\sqrt{4},\\sqrt{5},\\sqrt{6},\\sqrt{7},\\sqrt{8},\\sqrt{9},\\sqrt{10},\\sqrt{11},\\sqrt{12},\\sqrt{13},\\sqrt{14},\\sqrt{15},\\sqrt{16}$\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many vertical asymptotes does the graph of $y=\\\\frac{2}{x^2+x-6}$ have?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# the model generate new tokens\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=2048)\n",
    "# convert the generated tokens to text\n",
    "generated_text = tokenizer.decode(\n",
    "    output[0][len(inputs['input_ids'][0]) :], skip_special_tokens=True\n",
    ")\n",
    "print(\"\\nGenerated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the correct answer is 400, this demonstrates that there is still room for improvement in Llama 3.1's mathematical capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Model Using the GRPO Algorithm\n",
    "\n",
    "**Note**: If you cannot access huggingface.co, set the Hugging Face endpoint to hf-mirror.com. You can do this with the following command:\n",
    "\n",
    "`export HF_ENDPOINT=\"https://hf-mirror.com\"`\n",
    "\n",
    "Here, we take the PKU-SafeRLHF series dataset as an example. The PKU-SafeRLHF dataset is a preference dataset focused on safety alignment. Each data entry in this dataset contains two responses to the same question, along with their corresponding safety meta-tags and preference annotations.\n",
    "\n",
    "You can refer to the training script below:\n",
    "\n",
    "```bash\n",
    "# NOTE need to start the remote rm server first\n",
    "bash start_remote_rm.sh\n",
    "\n",
    "# NOTE need to change the model path\n",
    "ACTOR_MODEL_NAME_OR_PATH=\"meta-llama/Llama-3.1-8B-Instruct\" # actor model path\n",
    "\n",
    "TRAIN_DATASETS=\"../align_anything/models/remote_rm/math_verify_dataset/mathvl_345_example.json\" # dataset path\n",
    "TRAIN_TEMPLATE=\"Math-Zero-RL\" # math zero rlhf dataset template, note that for math zero rl, you are recommended to expand token length to longer length such as 18000\n",
    "TRAIN_SPLIT=\"train\" # split the input dataset\n",
    "\n",
    "OUTPUT_DIR=\"../output/llama_grpo_remote_rm\" # output dir\n",
    "# For wandb online logging\n",
    "export WANDB_API_KEY=\"\"\n",
    "\n",
    "export REMOTE_RM_URL=\"http://127.0.0.1:6000/get_reward\"\n",
    "# Source the setup script\n",
    "source ./setup.sh\n",
    "\n",
    "# Execute deepspeed command\n",
    "deepspeed \\\n",
    "  --master_port ${MASTER_PORT} \\\n",
    "  --module align_anything.trainers.text_to_text.grpo_remote_rm \\\n",
    "  --actor_model_name_or_path ${ACTOR_MODEL_NAME_OR_PATH} \\\n",
    "  --remote_rm_url ${REMOTE_RM_URL} \\\n",
    "  --train_datasets ${TRAIN_DATASETS} \\\n",
    "  --train_split ${TRAIN_SPLIT} \\\n",
    "  --train_template ${TRAIN_TEMPLATE} \\\n",
    "  --output_dir ${OUTPUT_DIR}\n",
    "```\n",
    "\n",
    "After training is completed, you can find the trained model weights under the `OUTPUT_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Performance of the Model After GRPO Training\n",
    "\n",
    "After the training is complete, we try to test whether the math of the trained model has improved.\n",
    "\n",
    "### 5.1 Load the New Model Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128257, 4096, padding_idx=128256)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/PATH/TO/YOUR/TRAINED_MODEL\"  # Please replace with your actual model path\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 测试新模型的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text: To find out how many terms are less than or equal to $20$, we can find out which term is greater than $20$, and then subtract $1$ to find the answer.\n",
      "\n",
      "Recognize that $\\sqrt{400} = 20$.\n",
      "\n",
      "The sequence goes by consecutive integers (1, 2, 3, 4, ect), so $\\sqrt{400}$ will be the 400th term.\n",
      "\n",
      "Thus, we can say every term up to the 400th term is less than or equal to $20$, except $\\sqrt{400}$.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many vertical asymptotes does the graph of $y=\\\\frac{2}{x^2+x-6}$ have?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# the model generate new tokens\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=2048)\n",
    "# convert the generated tokens to text\n",
    "generated_text = tokenizer.decode(\n",
    "    output[0][len(inputs['input_ids'][0]) :], skip_special_tokens=True\n",
    ")\n",
    "print(\"\\nGenerated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the fine-tuned model did indeed solve the problem correctly.\n",
    "\n",
    "(Strictly speaking, the test question was from the training dataset, so this is an in-distribution test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Customizing Reward Functions\n",
    "In this section, we will learn how to customize reward functions, allowing you to design specific scoring mechanisms based on your task requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Creating Reward Function Files\n",
    "First, create a new reward function file in the reward_functions directory of the project:\n",
    "```bash\n",
    "cd align-anything/align_anything/models/remote_rm/reward_functions/\n",
    "touch my_verifier.py\n",
    "```\n",
    "We can refer to the examples in examples.py to implement our own reward function. In this example, we'll implement a simple format verification reward function that focuses on whether the answer format is correct, without considering the accuracy of the answer.\n",
    "Here's the specific implementation code\n",
    "```python\n",
    "# align_anything/models/remote_rm/reward_functions/my_verifier.py\n",
    "import random\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "from flask import jsonify\n",
    "\n",
    "format_pattern = r'^<think>(?:(?!</think>).)*</think><answer>(?:(?!</answer>).)*</answer>\\Z'\n",
    "\n",
    "\n",
    "def verify_format(content):\n",
    "    \"\"\"\n",
    "    Verify if the string meets the format requirements:\n",
    "    - Must start with <think> and end with </answer>\n",
    "    - Must contain exactly one pair of <think>...</think> and <answer>...</answer> tags\n",
    "    - No extra characters allowed between </think> and <answer> tags\n",
    "    \"\"\"\n",
    "    think_count = content.count('<think>')\n",
    "    answer_count = content.count('<answer>')\n",
    "    return (\n",
    "        bool(re.match(format_pattern, content, re.DOTALL))\n",
    "        and think_count == 1\n",
    "        and answer_count == 1\n",
    "    )\n",
    "\n",
    "def my_verifier_reward_function(\n",
    "    prompts: List[str], responses: List[str], golden_responses: Optional[List[str]] = None\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Math verifier reward function, evaluate the accuracy of the answer\n",
    "\n",
    "    Args:\n",
    "        prompts: List of math problems\n",
    "        responses: List of model answers\n",
    "        golden_responses: Optional list of golden responses\n",
    "    Returns:\n",
    "        List of reward scores for each (prompt, response) pair\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    format_rewards = []\n",
    "    for prompt, response, golden_response in zip(prompts, responses, golden_responses):\n",
    "        if prompt is None:\n",
    "            return jsonify({'error': f'problem not found from {prompt}'}), 400\n",
    "        if golden_response is None:\n",
    "            return jsonify({'error': f'golden response not found from {prompt}'}), 400\n",
    "        # TODO: processing the error code 400\n",
    "\n",
    "        format_reward = float(verify_format(response))\n",
    "        rewards.append(format_reward)\n",
    "        format_rewards.append(format_reward)\n",
    "\n",
    "        do_print = random.randint(1, 10) == 1\n",
    "        if do_print:\n",
    "            info = f'Query: {prompt}\\n\\nAnswer: {golden_response}\\n\\nResponse: {response}\\n\\nFormat Reward: {format_reward}\\n\\n'\n",
    "            info = re.sub(r'<\\|.*?\\|>', '', info)\n",
    "            print(info)\n",
    "    return rewards\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Registering Custom Reward Functions\n",
    "\n",
    "After implementing the reward function, you need to register it in the framework:\n",
    "\n",
    "1. Add the following to `align_anything/models/remote_rm/reward_functions/__init__.py`:\n",
    "```python\n",
    "from .my_verifier import *\n",
    "```\n",
    "\n",
    "2. Register the function in `align_anything/models/remote_rm/run_reward_server.py`:\n",
    "```python\n",
    "reward_functions = {\n",
    "    'example_math': example_math_reward_function,\n",
    "    'example_coding': example_coding_reward_function,\n",
    "    'example_safety': example_safety_reward_function,\n",
    "    'math_verifier': math_verifier_reward_function,\n",
    "    'my_verifier': my_verifier_reward_function,\n",
    "}\n",
    "```\n",
    "\n",
    "3. Modify the configuration in `scripts/start_remote_rm.sh`:\n",
    "```bash\n",
    "export REWARD_TYPE=\"my_verifier\"\n",
    "```\n",
    "\n",
    "With this, the custom reward function configuration is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Training with Custom Reward Functions\n",
    "We use the same training command, but now our custom reward function is calculating the rewards behind the scenes\n",
    "\n",
    "```bash\n",
    "# NOTE need to start the remote rm server first\n",
    "bash start_remote_rm.sh\n",
    "\n",
    "# NOTE need to change the model path\n",
    "ACTOR_MODEL_NAME_OR_PATH=\"meta-llama/Llama-3.1-8B-Instruct\" # actor model path\n",
    "\n",
    "TRAIN_DATASETS=\"../align_anything/models/remote_rm/math_verify_dataset/mathvl_345_example.json\" # dataset path\n",
    "TRAIN_TEMPLATE=\"Math-Zero-RL\" # math zero rlhf dataset template, note that for math zero rl, you are recommended to expand token length to longer length such as 18000\n",
    "TRAIN_SPLIT=\"train\" # split the input dataset\n",
    "\n",
    "OUTPUT_DIR=\"../output/llama_grpo_remote_rm\" # output dir\n",
    "# For wandb online logging\n",
    "export WANDB_API_KEY=\"\"\n",
    "\n",
    "export REMOTE_RM_URL=\"http://127.0.0.1:6000/get_reward\"\n",
    "# Source the setup script\n",
    "source ./setup.sh\n",
    "\n",
    "# Execute deepspeed command\n",
    "deepspeed \\\n",
    "  --master_port ${MASTER_PORT} \\\n",
    "  --module align_anything.trainers.text_to_text.grpo_remote_rm \\\n",
    "  --actor_model_name_or_path ${ACTOR_MODEL_NAME_OR_PATH} \\\n",
    "  --remote_rm_url ${REMOTE_RM_URL} \\\n",
    "  --train_datasets ${TRAIN_DATASETS} \\\n",
    "  --train_split ${TRAIN_SPLIT} \\\n",
    "  --train_template ${TRAIN_TEMPLATE} \\\n",
    "  --output_dir ${OUTPUT_DIR}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Checking Reward Outputs\n",
    "\n",
    "To prevent reward hacking (where the model exploits loopholes in the reward function), we need to verify if the model's behavior meets expectations:\n",
    "\n",
    "1. Check the reward server logs:\n",
    "```bash\n",
    "tail -f align-anything/debug_logs/reward_server.log\n",
    "```\n",
    "\n",
    "If any anomalies are detected, adjust the reward function's evaluation logic promptly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Acknowledgements\n",
    "\n",
    "- [Hugging Face Transformers 文档](https://huggingface.co/docs/transformers/index)\n",
    "- [GRPO Paper](https://arxiv.org/pdf/2402.03300)\n",
    "- [DeepSeek-R1 Paper](https://arxiv.org/abs/2501.12948)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jy-align",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
