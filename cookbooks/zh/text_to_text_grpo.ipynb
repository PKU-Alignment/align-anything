{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 GRPO、RLVR 算法微调模型\n",
    "\n",
    "本教程将演示如何使用 **Group Relative Policy Optimization (GRPO)** 算法微调大型语言模型（以 **Llama-3.1-8B-Instruct** 模型为例）。通过本教程，你将学习如何在 **Align Anything** 框架下，为你的任务自定义奖励函数（reward function），并结合 **Reinforcement Learning with Verifiable Rewards (RLVR)** 方法，进一步提升模型在特定任务上的性能。\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 什么是 GRPO 算法？\n",
    "\n",
    "**Group Relative Policy Optimization (GRPO)** 是一种强化学习算法，旨在通过分组和相对奖励机制提升模型的推理能力。GRPO 最早在论文 *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models* 中提出，并在 DeepSeek-R1 的后训练阶段中被成功应用。\n",
    "\n",
    "GRPO 的目标是通过相对比较策略（而非绝对奖励）来优化模型的行为。具体而言，GRPO 会将多个模型输出分组，并根据它们的相对表现计算奖励值。这种方法能够缓解传统强化学习中绝对奖励难以定义或不够精确的问题，尤其适用于复杂推理任务。\n",
    "\n",
    "\n",
    "## 1.2 什么是 RLVR？\n",
    "\n",
    "**Reinforcement Learning with Verifiable Rewards (RLVR)** 是一种新颖的语言模型训练方法，专为具有可验证结果的任务（如数学问题求解和指令跟随）设计。RLVR 使用现有的强化学习奖励机制（如 RLHF），但用一种验证函数替代传统的奖励模型。\n",
    "\n",
    "与传统方法不同，RLVR 通过答案匹配或约束验证（例如答案是否正确）作为二元信号训练模型。当应用于数学领域或其他可验证任务时，RLVR 不仅能够提升特定基准（如 GSM8K）的性能，还能在其他任务中保持稳定表现。\n",
    "\n",
    "可以将 RLVR 看作是现有方法的简化版本，比如基于执行反馈的强化学习（RL with execution feedback）或语言模型推理的自举方法。它的核心思想是利用可验证信号作为直接奖励，避免了构建复杂奖励模型的繁琐过程。\n",
    "\n",
    "\n",
    "## 2. 环境配置\n",
    "\n",
    "在开始之前，请确保您已安装 ``align-anything`` 包。\n",
    "\n",
    "```bash\n",
    "# 克隆仓库\n",
    "git clone git@github.com:PKU-Alignment/align-anything.git\n",
    "cd align-anything\n",
    "\n",
    "# 使用conda创建虚拟环境\n",
    "conda create -n align-anything python==3.11\n",
    "conda activate align-anything\n",
    "```\n",
    "\n",
    "- **`[Optional]`** We recommend installing [CUDA](https://anaconda.org/nvidia/cuda) in the conda environment and set the environment variable.\n",
    "\n",
    "```bash\n",
    "# 我们在 H800 计算集群上测试过，这个版本的 CUDA 效果很好。\n",
    "# 您可以根据计算集群的实际情况调整此版本。\n",
    "\n",
    "conda install nvidia/label/cuda-12.2.0::cuda\n",
    "export CUDA_HOME=$CONDA_PREFIX\n",
    "```\n",
    "\n",
    "> 如果您的 CUDA 安装在不同的位置，例如 `/usr/local/cuda/bin/nvcc`，您可以按如下方式设置环境变量：\n",
    "\n",
    "```bash\n",
    "export CUDA_HOME=\"/usr/local/cuda\"\n",
    "```\n",
    "\n",
    "接着通过以下命令安装 `align-anything`：\n",
    "\n",
    "```bash\n",
    "# 我们为训练和评估准备了快速安装。\n",
    "# 如果您只需要使用训练或评估模块，\n",
    "# 您可以安装相应的依赖项。\n",
    "pip install -e .[train] # 安装训练依赖项\n",
    "pip install -e .[evaluate] # 安装评估依赖项\n",
    "\n",
    "# 如果您需要安装所有依赖项，可以使用以下命令：\n",
    "pip install -e .[all]\n",
    "```\n",
    "\n",
    "最后, 参照 https://github.com/PKU-Alignment/align-anything/tree/main/align_anything/models/remote_rm\n",
    "\n",
    "您还需要:\n",
    "```bash\n",
    "pip install Levenshtein flask latex2sympy2_extended math_verify\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Llama-3.1-8B-Instruct模型输出示例\n",
    "下面，让我们首先测试Llama-3.1-8B-Instruct模型的zero-shot能力。\n",
    "### 3.1 导入所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1742778596.498488] [dsw-519274-66f65ff576-678dh:4051137:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 加载原始的Llama 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"  # 将device设置为\"cuda\"以使用GPU\n",
    "model_path = \"/PATH/TO/YOUR/Llama-3.1-8B-Instruct\"  # 请更换为实际的模型路径\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# 将模型设置为eval模式\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 测试原始模型的性能\n",
    "\n",
    "让我们用一个示例问题测试 Llama-3.1-8B-Instruct 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text: The sequence of square roots of the positive integers is increasing. The largest term of the sequence that is less than or equal to 20 is $\\sqrt{19}$, the square root of 16. Therefore, 16 terms of the sequence are less than or equal to 20. The sequence of 16 terms is\n",
      "\n",
      "$\\sqrt{1},\\sqrt{2},\\sqrt{3},\\sqrt{4},\\sqrt{5},\\sqrt{6},\\sqrt{7},\\sqrt{8},\\sqrt{9},\\sqrt{10},\\sqrt{11},\\sqrt{12},\\sqrt{13},\\sqrt{14},\\sqrt{15},\\sqrt{16}$\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many vertical asymptotes does the graph of $y=\\\\frac{2}{x^2+x-6}$ have?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# the model generate new tokens\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=2048)\n",
    "# convert the generated tokens to text\n",
    "generated_text = tokenizer.decode(\n",
    "    output[0][len(inputs['input_ids'][0]) :], skip_special_tokens=True\n",
    ")\n",
    "print(\"\\nGenerated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而正确答案是400, 由此可见，llama 3.1的数学能力仍有提升的空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用GRPO算法对齐模型\n",
    "\n",
    "**注意**：如果您无法访问huggingface.co，请将huggingface的endpoint设置为hf-mirror.com。您可以进行以下操作：\n",
    "\n",
    "`export HF_ENDPOINT=\"https://hf-mirror.com\"`\n",
    "\n",
    "在这里，我们以 **Align Anything** 框架自带的示例数据集 mathvl_345_example.json 为例。mathvl_345_example 是一个简单数学数据集, 包含了10个数学问题和答案\n",
    "\n",
    "可以参考如下的训练脚本：\n",
    "\n",
    "```bash\n",
    "# NOTE need to start the remote rm server first\n",
    "bash start_remote_rm.sh\n",
    "\n",
    "# NOTE need to change the model path\n",
    "ACTOR_MODEL_NAME_OR_PATH=\"meta-llama/Llama-3.1-8B-Instruct\" # actor model path\n",
    "\n",
    "TRAIN_DATASETS=\"../align_anything/models/remote_rm/math_verify_dataset/mathvl_345_example.json\" # dataset path\n",
    "TRAIN_TEMPLATE=\"Math-Zero-RL\" # math zero rlhf dataset template, note that for math zero rl, you are recommended to expand token length to longer length such as 18000\n",
    "TRAIN_SPLIT=\"train\" # split the input dataset\n",
    "\n",
    "OUTPUT_DIR=\"../output/llama_grpo_remote_rm\" # output dir\n",
    "# For wandb online logging\n",
    "export WANDB_API_KEY=\"\"\n",
    "\n",
    "export REMOTE_RM_URL=\"http://127.0.0.1:6000/get_reward\"\n",
    "# Source the setup script\n",
    "source ./setup.sh\n",
    "\n",
    "# Execute deepspeed command\n",
    "deepspeed \\\n",
    "  --master_port ${MASTER_PORT} \\\n",
    "  --module align_anything.trainers.text_to_text.grpo_remote_rm \\\n",
    "  --actor_model_name_or_path ${ACTOR_MODEL_NAME_OR_PATH} \\\n",
    "  --remote_rm_url ${REMOTE_RM_URL} \\\n",
    "  --train_datasets ${TRAIN_DATASETS} \\\n",
    "  --train_split ${TRAIN_SPLIT} \\\n",
    "  --train_template ${TRAIN_TEMPLATE} \\\n",
    "  --output_dir ${OUTPUT_DIR}\n",
    "```\n",
    "\n",
    "训练完成后，您可以在`OUTPUT_DIR`下找到训练的模型权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 测试GRPO训练后的模型性能\n",
    "\n",
    "在训练结束后，我们试图测试训练后的模型数学能力是否有所改观。\n",
    "\n",
    "### 5.1 加载新的模型权重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128257, 4096, padding_idx=128256)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/PATH/TO/YOUR/TRAINED_MODEL\"  # 请更换为实际的模型路径\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# 将模型设置为eval模式\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 测试新模型的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text: To find out how many terms are less than or equal to $20$, we can find out which term is greater than $20$, and then subtract $1$ to find the answer.\n",
      "\n",
      "Recognize that $\\sqrt{400} = 20$.\n",
      "\n",
      "The sequence goes by consecutive integers (1, 2, 3, 4, ect), so $\\sqrt{400}$ will be the 400th term.\n",
      "\n",
      "Thus, we can say every term up to the 400th term is less than or equal to $20$, except $\\sqrt{400}$.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many vertical asymptotes does the graph of $y=\\\\frac{2}{x^2+x-6}$ have?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# the model generate new tokens\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=2048)\n",
    "# convert the generated tokens to text\n",
    "generated_text = tokenizer.decode(\n",
    "    output[0][len(inputs['input_ids'][0]) :], skip_special_tokens=True\n",
    ")\n",
    "print(\"\\nGenerated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可见，训练后的模型确实答对了问题. \n",
    "\n",
    "(当然严格来说, 测试的题目是在训练数据集里的, 这是 in distribution test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 自定义奖励函数\n",
    "在本节中,我们将学习如何自定义奖励函数,以便您可以根据具体任务需求设计专属的评分机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 创建奖励函数文件\n",
    "首先需要在项目的reward_functions目录下创建新的奖励函数文件:\n",
    "```bash\n",
    "cd align-anything/align_anything/models/remote_rm/reward_functions/\n",
    "touch my_verifier.py\n",
    "```\n",
    "我们可以参考examples.py中的示例来实现自己的奖励函数。本示例中,我们实现了一个简单的格式验证奖励函数,主要关注回答的格式是否正确,暂不考虑答案的准确性。\n",
    "下面是具体实现代码\n",
    "```python\n",
    "# align_anything/models/remote_rm/reward_functions/my_verifier.py\n",
    "import random\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "from flask import jsonify\n",
    "\n",
    "format_pattern = r'^<think>(?:(?!</think>).)*</think><answer>(?:(?!</answer>).)*</answer>\\Z'\n",
    "\n",
    "\n",
    "def verify_format(content):\n",
    "    \"\"\"\n",
    "    Verify if the string meets the format requirements:\n",
    "    - Must start with <think> and end with </answer>\n",
    "    - Must contain exactly one pair of <think>...</think> and <answer>...</answer> tags\n",
    "    - No extra characters allowed between </think> and <answer> tags\n",
    "    \"\"\"\n",
    "    think_count = content.count('<think>')\n",
    "    answer_count = content.count('<answer>')\n",
    "    return (\n",
    "        bool(re.match(format_pattern, content, re.DOTALL))\n",
    "        and think_count == 1\n",
    "        and answer_count == 1\n",
    "    )\n",
    "\n",
    "def my_verifier_reward_function(\n",
    "    prompts: List[str], responses: List[str], golden_responses: Optional[List[str]] = None\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Math verifier reward function, evaluate the accuracy of the answer\n",
    "\n",
    "    Args:\n",
    "        prompts: List of math problems\n",
    "        responses: List of model answers\n",
    "        golden_responses: Optional list of golden responses\n",
    "    Returns:\n",
    "        List of reward scores for each (prompt, response) pair\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    format_rewards = []\n",
    "    for prompt, response, golden_response in zip(prompts, responses, golden_responses):\n",
    "        if prompt is None:\n",
    "            return jsonify({'error': f'problem not found from {prompt}'}), 400\n",
    "        if golden_response is None:\n",
    "            return jsonify({'error': f'golden response not found from {prompt}'}), 400\n",
    "        # TODO: processing the error code 400\n",
    "\n",
    "        format_reward = float(verify_format(response))\n",
    "        rewards.append(format_reward)\n",
    "        format_rewards.append(format_reward)\n",
    "\n",
    "        do_print = random.randint(1, 10) == 1\n",
    "        if do_print:\n",
    "            info = f'Query: {prompt}\\n\\nAnswer: {golden_response}\\n\\nResponse: {response}\\n\\nFormat Reward: {format_reward}\\n\\n'\n",
    "            info = re.sub(r'<\\|.*?\\|>', '', info)\n",
    "            print(info)\n",
    "    return rewards\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 注册自定义奖励函数\n",
    "\n",
    "完成奖励函数实现后,需要将其注册到框架中:\n",
    "\n",
    "1. 在`align_anything/models/remote_rm/reward_functions/__init__.py`添加:\n",
    "```python\n",
    "from .my_verifier import *\n",
    "```\n",
    "\n",
    "2. 在`align_anything/models/remote_rm/run_reward_server.py`中注册函数:\n",
    "```python\n",
    "reward_functions = {\n",
    "    'example_math': example_math_reward_function,\n",
    "    'example_coding': example_coding_reward_function,\n",
    "    'example_safety': example_safety_reward_function,\n",
    "    'math_verifier': math_verifier_reward_function,\n",
    "    'my_verifier': my_verifier_reward_function,\n",
    "}\n",
    "```\n",
    "\n",
    "3. 修改`scripts/start_remote_rm.sh`中的配置:\n",
    "```bash\n",
    "export REWARD_TYPE=\"my_verifier\"\n",
    "```\n",
    "\n",
    "至此,自定义奖励函数配置完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 在自定义奖励函数后训练\n",
    "我们通过同样的命令进行训练, 但背后是我们的自定义奖励函数在计算reward\n",
    "\n",
    "```bash\n",
    "# NOTE need to start the remote rm server first\n",
    "bash start_remote_rm.sh\n",
    "\n",
    "# NOTE need to change the model path\n",
    "ACTOR_MODEL_NAME_OR_PATH=\"meta-llama/Llama-3.1-8B-Instruct\" # actor model path\n",
    "\n",
    "TRAIN_DATASETS=\"../align_anything/models/remote_rm/math_verify_dataset/mathvl_345_example.json\" # dataset path\n",
    "TRAIN_TEMPLATE=\"Math-Zero-RL\" # math zero rlhf dataset template, note that for math zero rl, you are recommended to expand token length to longer length such as 18000\n",
    "TRAIN_SPLIT=\"train\" # split the input dataset\n",
    "\n",
    "OUTPUT_DIR=\"../output/llama_grpo_remote_rm\" # output dir\n",
    "# For wandb online logging\n",
    "export WANDB_API_KEY=\"\"\n",
    "\n",
    "export REMOTE_RM_URL=\"http://127.0.0.1:6000/get_reward\"\n",
    "# Source the setup script\n",
    "source ./setup.sh\n",
    "\n",
    "# Execute deepspeed command\n",
    "deepspeed \\\n",
    "  --master_port ${MASTER_PORT} \\\n",
    "  --module align_anything.trainers.text_to_text.grpo_remote_rm \\\n",
    "  --actor_model_name_or_path ${ACTOR_MODEL_NAME_OR_PATH} \\\n",
    "  --remote_rm_url ${REMOTE_RM_URL} \\\n",
    "  --train_datasets ${TRAIN_DATASETS} \\\n",
    "  --train_split ${TRAIN_SPLIT} \\\n",
    "  --train_template ${TRAIN_TEMPLATE} \\\n",
    "  --output_dir ${OUTPUT_DIR}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 检查奖励输出\n",
    "\n",
    "为防止reward hacking(奖励函数被模型钻空子),需要检查模型行为是否符合预期:\n",
    "\n",
    "1. 查看reward server日志:\n",
    "```bash\n",
    "tail -f align-anything/debug_logs/reward_server.log\n",
    "```\n",
    "\n",
    "如发现异常,及时调整奖励函数的判定逻辑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 致谢\n",
    "\n",
    "- [Hugging Face Transformers 文档](https://huggingface.co/docs/transformers/index)\n",
    "- [GRPO 论文](https://arxiv.org/pdf/2402.03300)\n",
    "- [DeepSeek-R1 论文](https://arxiv.org/abs/2501.12948)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jy-align",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
