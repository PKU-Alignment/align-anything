# Copyright 2024 PKU-Alignment Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================


# training configurations
train_cfgs:
  # The deepspeed configuration
  ds_cfgs: ds_z3_config.json
  # Number of training epochs
  epochs: 3
  # Seed for random number generator
  seed: 42
  # Batch size per device for training
  per_device_train_batch_size: 2
  # Batch size per device for evaluation
  per_device_eval_batch_size: 2
  # The number of gradient accumulation steps
  gradient_accumulation_steps: 1
  # Whether to use gradient checkpointing
  gradient_checkpointing: True
  # Initial learning rate
  learning_rate: 5.e-6
  # Type of learning rate scheduler
  lr_scheduler_type: cosine
  # Ratio of warmup steps for learning rate
  lr_warmup_ratio: 0.03
  # Weight decay coefficient
  weight_decay: 0.0
  # Hyper-parameters for adam optimizer
  adam_betas: [0.9, 0.95]
  # Enable bfloat 16 precision
  bf16: True
  # Enable float 16 precision
  fp16: False
  # The strategy of evaluation, choosing form [epoch, steps]
  eval_strategy: epoch
  # The evaluation interval in step-wise evaluation case
  eval_interval: 10
  # The reward modeling regularization
  regularization: 0.001
  # The scale coefficient
  scale_coeff: 0.5
# Configuration for datasets
data_cfgs:
  # Datasets to use for training
  train_datasets: null
  # The format template for training
  train_template: null
  # The total number for training
  train_size: null
  # The split of train datasets
  train_split: null
  # The subset of training datasets
  train_subset: null
  # The training data files to be used
  train_data_files: null
  # The optional arguments for loading training datasets
  train_optional_args: []
  # Datasets to use for evaluation
  eval_datasets: null
  # The format template for evaluation
  eval_template: null
  # The total number for evaluation
  eval_size: null
  # The split of evaluation datasets
  eval_split: null
  # The subset of evaluation datasets
  eval_subset: null
  # The evaluation data files to be used
  eval_data_files: null
  # The optional arguments for loading training evaluation datasets
  eval_optional_args: []
# Configuration for logging
logger_cfgs:
  # Type of logging to use, choosing from [wandb, tensorboard]
  log_type: wandb
  # Project name for logging
  log_project: align-anything
  # Run name for logging
  log_run_name: orpo
  # Output directory name
  output_dir: null
  # The directory to cache the downloaded model
  cache_dir: null
  # The interval of saving models
  save_interval: 100000
# Model configurations
model_cfgs:
  # Pretrained model name or path
  model_name_or_path: null
  # Whether to trust remote code
  trust_remote_code: True
  # The max token length
  model_max_length: 1024
# Configuration for training with LoRA
lora_cfgs:
  # Whether to use LoRA
  use_lora: False
  # Task type for LoRA configuration
  task_type: TaskType.CAUSAL_LM
  # Inference mode
  inference_mode: False
  # Rank of the low-rank adaptation matrices
  r: 16
  # Alpha parameter for LoRA
  lora_alpha: 16
  # Dropout rate for LoRA
  lora_dropout: 0.1
  # Target modules for applying LoRA
  target_modules: ["q_proj", "v_proj"]
  # Whether to save the full model
  save_full_model: True
# Configuration for training with QLoRA
bnb_cfgs:
  # Whether to use BNB(For QLoRA)
  use_bnb: False
  # Whether to use 4-bit quantization
  load_in_4bit: True
  # Whether to use 8-bit quantization
  load_in_8bit: False
  # The quantization type for 4-bit quantization
  bnb_4bit_quant_type: nf4
  # Whether to use double quantization
  bnb_4bit_use_double_quant: True
  # The compute dtype for 4-bit quantization
  bnb_4bit_compute_dtype: float16
# Customized special tokens
special_tokens: null
