# Copyright 2024 PKU-Alignment Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

# training configurations
train_cfgs:
  # The deepspeed configuration
  ds_cfgs: ds_z3_config.json
  # Number of training epochs
  epochs: 3
  # Seed for random number generator
  seed: 42
  # Batch size per device for training
  per_device_train_batch_size: 4
  # Batch size per device for evaluation
  per_device_eval_batch_size: 4
  # The number of gradient accumulation steps
  gradient_accumulation_steps: 2
  # Whether to use gradient checkpointing
  gradient_checkpointing: False
  # Initial learning rate
  learning_rate: 1.e-5
  # Type of learning rate scheduler
  lr_scheduler_type: constant
  # Ratio of warmup steps for learning rate
  lr_warmup_ratio: 0.03
  # Weight decay coefficient
  weight_decay: 0.01
  # Hyper-parameters for adam optimizer
  adam_betas: [0.9, 0.999]
  # Hyper-parameters for adam epsilon
  adam_epsilon: 1.e-8
  # Enable bfloat 16 precision
  bf16: True
  # Enable float 16 precision
  fp16: False
  # The strategy of evaluation, choosing form [epoch, steps]
  eval_strategy: epoch
  # The evaluation interval in step-wise evaluation case
  eval_interval: 10
  # The reward modeling regularization
  regularization: 0.001
  # Batch size to use for VAE encoding of the images for efficient processing.
  vae_encode_batch_size: 8
  # DPO KL Divergence penalty coefficient.
  beta_coeff: 2500
  # The type of loss function, chosen from [sigmoid, hinge]
  loss_type: sigmoid
  # Freeze the unet model
  freeze_unet: False
  # Enable the unet LoRA training
  lora_unet: False
  # The max norm of gradient
  max_grad_norm: 1.0
  # The resolution of the video processor
  resolution: 512
  # The number of frames to sample
  sample_frames: 16
  # Whether to do resize for the input frames
  do_resize: True
# Configuration for datasets
data_cfgs:
  # Datasets to use for training
  train_datasets: null
  # The format template for training
  train_template: null
  # The total number for training
  train_size: null
  # The split of train datasets
  train_split: null
  # The optional arguments for loading training datasets
  train_optional_args: []
  # Datasets to use for evaluation
  eval_datasets: null
  # The format template for evaluation
  eval_template: null
  # The total number for evaluation
  eval_size: null
  # The split of evaluation datasets
  eval_split: null
  # The optional arguments for loading training evaluation datasets
  eval_optional_args: []
# Configuration for logging
logger_cfgs:
  # Type of logging to use, choosing from [wandb, tensorboard]
  log_type: wandb
  # Project name for logging
  log_project: align-anything
  # Run name for logging
  log_run_name: dpo
  # Output directory name
  output_dir: null
  # The directory to cache the downloaded model
  cache_dir: null
  # The interval of saving models
  save_interval: 100000
# Model configurations
model_cfgs:
  # Pretrained model name or path
  model_name_or_path: null
  # Whether to trust remote code
  trust_remote_code: True
  # The max token length
  model_max_length: 512
  # Whether
# Customized special tokens
special_tokens: null
