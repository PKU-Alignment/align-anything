# Copyright 2025 PKU-Alignment Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

# The training configurations
train_cfgs:
  # The deepspeed configuration file for optimizing the training process
  ds_cfgs: ds_z3_config.json
  # Number of training epochs
  epochs: 3
  # Seed for random number generator to ensure reproducibility
  seed: 42
  # Batch size per device for prompt-based training
  per_device_prompt_batch_size: 2
  # Batch size per device for training the main model
  per_device_train_batch_size: 2
  # Batch size per device for evaluation
  per_device_eval_batch_size: 1
  # The number of gradient accumulation steps
  gradient_accumulation_steps: 1
  # Whether to use gradient checkpointing for the actor model
  actor_gradient_checkpointing: True
  # Whether to use gradient checkpointing for the critic model
  critic_gradient_checkpointing: True
  # Initial learning rate for the actor model
  actor_lr: 5.e-7
  # Type of learning rate scheduler for the actor model (e.g., "cosine", "linear", etc.)
  actor_lr_scheduler_type: cosine
  # Ratio of warmup steps for learning rate for the actor model
  actor_lr_warmup_ratio: 0.03
  # Weight decay coefficient for the actor model to prevent overfitting
  actor_weight_decay: 0.00
  # Initial learning rate for the critic model
  critic_lr: 5.e-7
  # Type of learning rate scheduler for the critic model
  critic_lr_scheduler_type: constant
  # Ratio of warmup steps for learning rate for the critic model
  critic_lr_warmup_ratio: 0.03
  # Weight decay coefficient for the critic model
  critic_weight_decay: 0.0
  # Hyper-parameters for the Adam optimizer (momentum terms)
  adam_betas: [0.9, 0.95]
  # Enable bfloat 16 precision for faster computations (using TPUs)
  bf16: True
  # Enable float 16 precision for faster training (using GPUs)
  fp16: False
  # The strategy for evaluation (choose between [epoch, steps])
  eval_strategy: epoch
  # The evaluation interval in steps (used in step-wise evaluation)
  eval_interval: 10
  # The coefficient for the KL divergence between the reference and actor policy
  kl_coeff: 0.02
  # The clipping range for the ratio between the old and new policy (PPO)
  clip_range_ratio: 0.2
  # The clipping range for the output of the score model
  clip_range_score: 50.0
  # The clipping range for the value function
  clip_range_value: 5.0
  # The coefficient for the PTX loss
  ptx_coeff: 16.0
  # The discount factor for the advantage function
  gamma: 1.0
  # The hyperparameter controlling the trade-off between bias and variance of the advantage function
  gae_lambda: 0.95
  # The initial value of the lambda coefficient
  lambda_init: 10
  # The maximum value for the lambda coefficient
  lambda_max: 20
  # The learning rate for updating the lambda coefficient
  lambda_lr: 0.1
  # Number of steps to delay lambda updates
  lambda_update_delay_steps: 1
  # Size of the window for episode cost
  episode_cost_window_size: 128
  # Whether to normalize the reward during RL training
  normalize_reward: False
  # The number of repeated updates on a generated batch
  update_iters: 1
  # Whether to freeze the multi-modal projection layer during training
  freeze_mm_proj: False
  # Whether to freeze the vision tower model during training
  freeze_vision_tower: True
  # Whether to freeze the language model during training
  freeze_language_model: False
  # Threshold value for custom decision-making logic
  threshold: -0.5

# Configuration for datasets
data_cfgs:
  # Datasets to use for training, specify as a list of dataset paths
  train_datasets: null
  # The format template for training datasets (e.g., JSON, CSV)
  train_template: null
  # The total number of training samples to use
  train_size: null
  # The split ratio of training datasets
  train_split: null
  # The subset of training datasets to use (e.g., first N examples)
  train_name: null
  # The specific training data files to be used
  train_data_files: null
  # Optional arguments for loading training datasets
  train_optional_args: []
  # Datasets to use for evaluation
  eval_datasets: null
  # The format template for evaluation datasets
  eval_template: null
  # The total number of evaluation samples to use
  eval_size: null
  # The split ratio of evaluation datasets
  eval_split: null
  # The subset of evaluation datasets to use
  eval_name: null
  # The specific evaluation data files to be used
  eval_data_files: null
  # Optional arguments for loading evaluation datasets
  eval_optional_args: []
  # Datasets to use for PTX loss computation
  ptx_datasets: null
  # The format template for PTX training datasets
  ptx_template: null
  # The total number of PTX training samples to use
  ptx_size: null
  # The subset of PTX datasets to use
  ptx_subset: null
  # The split ratio of PTX datasets
  ptx_split: null
  # The specific PTX training data files to be used
  ptx_data_files: null
  # Optional arguments for loading PTX training datasets
  ptx_optional_args: []

# Configuration for logging
logger_cfgs:
  # Type of logging to use, options: [wandb, tensorboard]
  log_type: wandb
  # Project name for logging
  log_project: align-anything
  # Run name for logging
  log_run_name: saferlhf
  # Output directory for saving logs and models
  output_dir: null
  # Directory to cache the downloaded models
  cache_dir: null
  # The interval of saving models during training (in steps)
  save_interval: 100000

# Model configurations
model_cfgs:
  # Pretrained model path or name for the actor model in RLHF
  actor_model_name_or_path: null
  # Pretrained model path or name for the reward model in RLHF
  reward_model_name_or_path: null
  # Pretrained model path or name for the critic model in RLHF
  reward_critic_model_name_or_path: null
  # Pretrained model path or name for the cost model
  cost_model_name_or_path: null
  # Pretrained model path or name for the cost critic model
  cost_critic_model_name_or_path: null
  # Whether to trust the remote code while loading models
  trust_remote_code: True
  # The maximum token length for input to the model
  model_max_length: 8192
  # The maximum number of new tokens to generate during inference
  max_new_tokens: 512
  # Temperature for controlling randomness in generation (higher = more random)
  temperature: 1.0
  # If set to float < 1, only the smallest set of most probable tokens with probabilities
  # that add up to `top_p` or higher are kept for generation.
  top_p: 1.0
  # The parameter for repetition penalty to prevent the model from repeating itself
  repetition_penalty: 1.0

# Customized special tokens, if any
special_tokens: null
