# Copyright 2024 Allen Institute for AI

# Copyright 2025 Align-Anything Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

# The training configurations
train_cfgs:
  # Whether to save the model checkpoint
  # if `False`, only save the 16-bits model
  save_checkpoint: False
  # Whether to load the model from checkpoint
  load_checkpoint: False
  # The deepspeed configuration
  ds_cfgs: ds_z3_config.json
  # Seed for random number generator
  seed: 3407
  # Number of training epochs
  epochs: 200
  # Batch size per device for training
  per_device_train_batch_size: 8
  # Batch size per device for validation
  per_device_val_batch_size: 12
  # The number of gradient accumulation steps
  gradient_accumulation_steps: 2
  # Whether to use gradient checkpointing
  gradient_checkpointing: False
  # Initial learning rate
  learning_rate: 0.0001
  # Type of learning rate scheduler
  lr_scheduler_type: constant
  # Ratio of warmup steps for learning rate
  lr_warmup_ratio: 0.0
  # Weight decay coefficient
  weight_decay: 0.0
  # Hyper-parameters for adam optimizer
  adam_betas: [0.9, 0.95]
  # Hyper-parameters for adam epsilon
  adam_epsilon: 1.e-8
  # Loss function
  loss: "action"
  # The strategy of validation, choosing form [epoch, steps]
  val_strategy: steps
  # The interval of validation
  val_interval: 500
  # The max norm of gradient
  max_grad_norm: 1.0
  # Enable bfloat 16 precision
  bf16: True
  # Enable float 16 precision
  fp16: False
# The data configurations
data_cfgs:
  # Datasets to use for training
  dataset_version: "CHORES"
  # Dataset Task type, such as PickupType, ObjectNavType, FetchType, RoomVisit, etc.
  dataset_task_type: ["PickupType"]
  # The data directory
  data_dir: "path/to/data"
  # The sliding window
  sliding_window: 100
  # The initial probability of sampling the last steps
  init_prob_sample_last_steps: 0.0
  # The final probability of sampling the last steps
  final_prob_sample_last_steps: 0.0
  # Whether to reduce action redundancy
  reduce_action_redundancy: False
  # The maximum number of samples for training
  max_samples: 10000000
  # The maximum number of samples for validation
  val_max_samples: 640
  # Whether to use validation datasets
  val_datasets: False
  # The number of workers
  num_workers: 8
sensor_cfgs:
  # The input sensors
  input_sensors: ["raw_navigation_camera", "raw_manipulation_camera"]
# The logging configurations
logger_cfgs:
  # Type of logging to use, choosing from [wandb, tensorboard]
  log_type: "wandb"
  # Project name for logging
  log_project: null
  # Run name for logging
  log_run_name: null
  # Output directory name
  output_dir: null
  # The directory to cache the downloaded model
  cache_dir: null
  # The interval of saving models
  save_total_limit: 1
  # The interval of saving videos
  log_video_interval: 2000
  # The interval of saving videos for validation
  log_video_interval_val: 100
# The model configurations
model_cfgs:
  # The model architecture
  model_architecture: "EarlyFusionCnnTransformer"
  # Pretrained model name or path(end with slice_xx), if null, use the model architecture to train model from scratch
  model_name_or_path: null
  # The model version
  model_version: "small_3"
# The LoRA configurations
lora_cfgs:
  # Whether to use LoRA
  use_lora: False
  # Task type for LoRA configuration
  task_type: TaskType.CAUSAL_LM
  # Inference mode
  inference_mode: False
  # Rank of the low-rank adaptation matrices
  r: 16
  # Alpha parameter for LoRA
  lora_alpha: 16
  # Dropout rate for LoRA
  lora_dropout: 0.1
  # Target modules for applying LoRA
  target_modules: ["q_proj", "v_proj"]
  # Whether to save the full model
  save_full_model: True
# The QLoRA configurations
bnb_cfgs:
  # Whether to use BNB(For QLoRA)
  use_bnb: False
  # Whether to use 4-bit quantization
  load_in_4bit: True
  # Whether to use 8-bit quantization
  load_in_8bit: False
  # The quantization type for 4-bit quantization
  bnb_4bit_quant_type: nf4
  # Whether to use double quantization
  bnb_4bit_use_double_quant: True
  # The compute dtype for 4-bit quantization
  bnb_4bit_compute_dtype: float16
# Customized special tokens
special_tokens: null
