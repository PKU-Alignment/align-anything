{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multimodal LLaVA Model\n",
    "\n",
    "This tutorial demonstrates how to build a multimodal model base on LLaVA architecture, by combining a language model and a vision model with a projection layer. LLaVA models can process both text and images, allowing for tasks like image captioning, visual question answering, and more.\n",
    "\n",
    "## What is LLaVA?\n",
    "\n",
    "LLaVA is a multimodal model architecture that connects a vision encoder with a language model, enabling the processing of both visual and textual information. This allows the model to understand and generate text based on image inputs.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, make sure you have installed the ``align-anything`` package.\n",
    "\n",
    "```bash\n",
    "# clone the repository\n",
    "git clone git@github.com:PKU-Alignment/align-anything.git\n",
    "cd align-anything\n",
    "\n",
    "# create virtual env\n",
    "conda create -n align-anything python==3.11\n",
    "conda activate align-anything\n",
    "```\n",
    "\n",
    "- **`[Optional]`** We recommend installing [CUDA](https://anaconda.org/nvidia/cuda) in the conda environment and set the environment variable.\n",
    "\n",
    "```bash\n",
    "# We tested on the H800 computing cluster, and this version of CUDA works well.\n",
    "# You can adjust this version according to the actual situation of the computing cluster.\n",
    "\n",
    "conda install nvidia/label/cuda-12.2.0::cuda\n",
    "export CUDA_HOME=$CONDA_PREFIX\n",
    "```\n",
    "\n",
    "> If your CUDA installed in a different location, such as `/usr/local/cuda/bin/nvcc`, you can set the environment variables as follows:\n",
    "\n",
    "```bash\n",
    "export CUDA_HOME=\"/usr/local/cuda\"\n",
    "```\n",
    "\n",
    "Finally, install `align-anything` by:\n",
    "\n",
    "```bash\n",
    "# We prepare quick installation for training and evaluation.\n",
    "# If you only need to use the training or evaluation module,\n",
    "# you can install the corresponding dependencies.\n",
    "pip install -e .[train] # install the training dependencies\n",
    "pip install -e .[evaluate] # install the evaluation dependencies\n",
    "\n",
    "# If you need to install all dependencies, you can use the following command:\n",
    "pip install -e .[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    CLIPImageProcessor,\n",
    "    LlavaConfig,\n",
    "    LlavaForConditionalGeneration,\n",
    "    LlavaProcessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function for Model Optimization\n",
    "\n",
    "The following function ensures that all model parameters are contiguous in memory, which can improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_contiguous(module):\n",
    "    \"\"\"Make all model parameters contiguous in memory for better performance.\"\"\"\n",
    "    for child in module.children():\n",
    "        make_model_contiguous(child)\n",
    "\n",
    "    for param in module.parameters(recurse=False):\n",
    "        param.data = param.data.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Model Paths\n",
    "\n",
    "You can customize these paths based on your preferences. By default, we'll use Meta-Llama-3.1-8B-Instruct as the language model and CLIP ViT-Large as the vision model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model paths\n",
    "language_model_path = (\n",
    "    \"/PATH/TO/YOUR/Llama-3.1-8B-Instruct\"  # You can change this to any compatible language model\n",
    ")\n",
    "vision_tower_path = (\n",
    "    \"/PATH/TO/YOUR/clip-vit-large-patch14-336\"  # You can change this to any compatible vision model\n",
    ")\n",
    "# TODO: change this to your own path\n",
    "save_path = \"/PATH/TO/YOUR/llama_vision\"  # Where to save the combined model\n",
    "\n",
    "# Create the save directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Base Models\n",
    "\n",
    "First, we'll load the vision model, language model, tokenizer, and image processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vision model\n",
    "print(\"Loading vision model...\")\n",
    "vision_model = AutoModel.from_pretrained(vision_tower_path)\n",
    "\n",
    "# Load the language model\n",
    "print(\"Loading language model...\")\n",
    "language_model = AutoModelForCausalLM.from_pretrained(language_model_path)\n",
    "\n",
    "# Load the tokenizer for the language model\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model_path)\n",
    "\n",
    "# Load the image processor for the vision model\n",
    "print(\"Loading image processor...\")\n",
    "image_processor = CLIPImageProcessor.from_pretrained(vision_tower_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Tokenizer\n",
    "\n",
    "We need to add special tokens to the tokenizer for handling images and padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<image>', '<unk>', '<pad>']})\n",
    "tokenizer.pad_token = '<pad>'\n",
    "tokenizer.unk_token = '<unk>'\n",
    "\n",
    "print(f\"Tokenizer vocabulary size after adding special tokens: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the LLaVA Configuration\n",
    "\n",
    "Now we'll create a configuration that combines the vision and language model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract configurations from the base models\n",
    "vision_config = vision_model.vision_model.config\n",
    "language_config = language_model.config\n",
    "\n",
    "# Create a combined LLaVA configuration\n",
    "config = LlavaConfig(vision_config, language_config)\n",
    "\n",
    "# Set the image token index in the configuration\n",
    "config.image_token_index = tokenizer.convert_tokens_to_ids('<image>')\n",
    "\n",
    "# Set the chat template as the original llava template\n",
    "# copy from https://huggingface.co/llava-hf/llava-1.5-7b-hf/blob/main/chat_template.json\n",
    "llava_template = \"\"\"\"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\\n' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\"\"\"\n",
    "\n",
    "# Create a processor that combines the image processor and tokenizer\n",
    "processor = LlavaProcessor(\n",
    "    image_processor=image_processor,\n",
    "    tokenizer=tokenizer,\n",
    "    patch_size=14,\n",
    "    chat_template=llava_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LLaVA Model\n",
    "\n",
    "Now we'll create the LLaVA model by combining the vision and language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLaVA model with the combined configuration\n",
    "print(\"Building LLaVA model...\")\n",
    "model = LlavaForConditionalGeneration(config)\n",
    "\n",
    "# Assign the pre-trained language model\n",
    "model.language_model = language_model\n",
    "\n",
    "# Assign the pre-trained vision model\n",
    "model.vision_tower = vision_model\n",
    "\n",
    "# Resize the token embeddings to match the new tokenizer size\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Make the model parameters contiguous for better performance\n",
    "make_model_contiguous(model)\n",
    "\n",
    "print(\"LLaVA model built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "\n",
    "Finally, we'll save the combined model and processor to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving the model, you may need to specify the `CUDA_HOME` environment variable to avoid cuda warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_HOME\"] = \"/PATH/TO/YOUR/CUDA\"  # run `which nvcc` to get the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and processor\n",
    "print(f\"Saving model to {save_path}...\")\n",
    "model.save_pretrained(save_path)\n",
    "processor.save_pretrained(save_path)\n",
    "print(\"Model and processor saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "Let's test our newly created LLaVA model with a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What are these?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first checkout the image and the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the model and processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and processor\n",
    "loaded_model = LlavaForConditionalGeneration.from_pretrained(save_path)\n",
    "loaded_processor = LlavaProcessor.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should process the image and the prompt to torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = loaded_processor(images=raw_image, text=prompt, return_tensors='pt')\n",
    "print('The input has the following keys: ', inputs.keys())\n",
    "print('Their shapes are: ', {k: v.shape for k, v in inputs.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = loaded_model.generate(**inputs, max_new_tokens=20, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the model's output is pretty messy. That's because the model is not trained on the image-text pair. As showcase in the LLaVA paper, we should fine-tune the model on the image-text pair to get a better performance. A reference script is provided in the `scripts/llava_step1.sh` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training setup is mainly based on the LLaVA paper.\n",
    "\n",
    "**Note**: You should finish the following steps to make sure your training process is smooth.\n",
    "\n",
    "1. Download the `liuhaotian/LLaVA-Pretrain` dataset by:\n",
    "\n",
    "```bash\n",
    "huggingface-cli download --repo-type dataset --resume-download liuhaotian/LLaVA-Pretrain --local-dir /PATH/TO/YOUR/liuhaotian/LLaVA-Pretrain --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "2. unzip the `images.zip` file and put the `images` folder in the `/PATH/TO/YOUR/images/` folder.\n",
    "\n",
    "3. Specify the `COCO_DATA_DIR` environment variable to the path of the `images` folder.\n",
    "\n",
    "The final script is as follows:\n",
    "\n",
    "```bash\n",
    "export COCO_DATA_DIR=\"/PATH/TO/YOUR/images/\"\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "MODEL_NAME_OR_PATH=\"/PATH/TO/YOUR/MLLMs\"\n",
    "TRAIN_DATASETS=\"/PATH/TO/YOUR/liuhaotian/LLaVA-Pretrain\"\n",
    "TRAIN_TEMPLATE=\"LLaVA_Pretrain\" # dataset template\n",
    "TRAIN_DATA_FILES=\"blip_laion_cc_sbu_558k.json\"\n",
    "\n",
    "OUTPUT_DIR=\"../outputs/llava_step1\" # output dir\n",
    "\n",
    "# For wandb online logging\n",
    "export WANDB_API_KEY=\"\"\n",
    "\n",
    "# Source the setup script\n",
    "source ./setup.sh\n",
    "\n",
    "# Execute deepspeed command\n",
    "deepspeed \\\n",
    "        --master_port ${MASTER_PORT} \\\n",
    "        --module align_anything.trainers.text_image_to_text.sft \\\n",
    "        --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n",
    "        --train_datasets ${TRAIN_DATASETS} \\\n",
    "        --train_template ${TRAIN_TEMPLATE} \\\n",
    "        --train_split train \\\n",
    "        --train_data_files ${TRAIN_DATA_FILES} \\\n",
    "        --output_dir ${OUTPUT_DIR} \\\n",
    "        --save_total_limit 3 \\\n",
    "        --freeze_vision_tower True \\\n",
    "        --freeze_mm_proj False \\\n",
    "        --freeze_language_model True \\\n",
    "        --epochs 1 \\\n",
    "        --ds_cfgs ds_z2_config.json \\\n",
    "        --learning_rate 1.e-3 \\\n",
    "        --per_device_train_batch_size 16 \\\n",
    "        --gradient_accumulation_steps 32\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully built a multimodal LLaVA model by combining a language model with a vision model. This model can now process both text and images, enabling a wide range of multimodal applications.\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [LLaVA Paper](https://arxiv.org/abs/2304.08485)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
