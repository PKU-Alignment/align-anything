{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建多模态 LLaVA 模型\n",
    "\n",
    "本教程演示如何基于 LLaVA 架构构建多模态模型，通过结合语言模型和视觉模型以及投影层。LLaVA 模型可以同时处理文本和图像，支持图像描述、视觉问答等任务。\n",
    "\n",
    "## 什么是 LLaVA？\n",
    "\n",
    "LLaVA 是一种多模态模型架构，它将视觉编码器与语言模型连接起来，使模型能够同时处理视觉和文本信息。这使得模型能够基于图像输入来理解和生成文本。\n",
    "\n",
    "## 前提条件\n",
    "\n",
    "在开始之前，请确保您已安装 ``align-anything`` 包。\n",
    "\n",
    "```bash\n",
    "# 克隆仓库\n",
    "git clone git@github.com:PKU-Alignment/align-anything.git\n",
    "cd align-anything\n",
    "\n",
    "# 使用conda创建虚拟环境\n",
    "conda create -n align-anything python==3.11\n",
    "conda activate align-anything\n",
    "```\n",
    "\n",
    "- **`[Optional]`** We recommend installing [CUDA](https://anaconda.org/nvidia/cuda) in the conda environment and set the environment variable.\n",
    "\n",
    "```bash\n",
    "# 我们在 H800 计算集群上测试过，这个版本的 CUDA 效果很好。\n",
    "# 您可以根据计算集群的实际情况调整此版本。\n",
    "\n",
    "conda install nvidia/label/cuda-12.2.0::cuda\n",
    "export CUDA_HOME=$CONDA_PREFIX\n",
    "```\n",
    "\n",
    "> 如果您的 CUDA 安装在不同的位置，例如 `/usr/local/cuda/bin/nvcc`，您可以按如下方式设置环境变量：\n",
    "\n",
    "```bash\n",
    "export CUDA_HOME=\"/usr/local/cuda\"\n",
    "```\n",
    "\n",
    "最后，通过以下命令安装 `align-anything`：\n",
    "\n",
    "```bash\n",
    "# 我们为训练和评估准备了快速安装。\n",
    "# 如果您只需要使用训练或评估模块，\n",
    "# 您可以安装相应的依赖项。\n",
    "pip install -e .[train] # 安装训练依赖项\n",
    "pip install -e .[evaluate] # 安装评估依赖项\n",
    "\n",
    "# 如果您需要安装所有依赖项，可以使用以下命令：\n",
    "pip install -e .[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    CLIPImageProcessor,\n",
    "    LlavaConfig,\n",
    "    LlavaForConditionalGeneration,\n",
    "    LlavaProcessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型优化的辅助函数\n",
    "\n",
    "以下函数确保所有模型参数在内存中是连续的，这可以提高性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_contiguous(module):\n",
    "    \"\"\"Make all model parameters contiguous in memory for better performance.\"\"\"\n",
    "    for child in module.children():\n",
    "        make_model_contiguous(child)\n",
    "\n",
    "    for param in module.parameters(recurse=False):\n",
    "        param.data = param.data.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置模型路径\n",
    "\n",
    "您可以根据自己的偏好自定义这些路径。默认情况下，我们将使用 Meta-Llama-3.1-8B-Instruct 作为语言模型，CLIP ViT-Large 作为视觉模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型路径\n",
    "language_model_path = \"/PATH/TO/YOUR/Llama-3.1-8B-Instruct\"  # 您可以更改此路径以使用任何兼容的语言模型\n",
    "vision_tower_path = \"/PATH/TO/YOUR/clip-vit-large-patch14-336\"  # 您可以更改此路径以使用任何兼容的视觉模型\n",
    "# TODO: 更改为自己的路径\n",
    "save_path = \"/PATH/TO/YOUR/llama_vision\"  # 保存组合模型的位置\n",
    "\n",
    "# 如果目录不存在，则创建目录\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载基础模型\n",
    "\n",
    "首先，我们将加载视觉模型、语言模型、分词器和图像处理器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载视觉模型\n",
    "print(\"Loading vision model...\")\n",
    "vision_model = AutoModel.from_pretrained(vision_tower_path)\n",
    "\n",
    "# 加载语言模型\n",
    "print(\"Loading language model...\")\n",
    "language_model = AutoModelForCausalLM.from_pretrained(language_model_path)\n",
    "\n",
    "# 加载语言模型的分词器\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model_path)\n",
    "\n",
    "# 加载视觉模型的图像处理器\n",
    "print(\"Loading image processor...\")\n",
    "image_processor = CLIPImageProcessor.from_pretrained(vision_tower_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备分词器\n",
    "\n",
    "我们需要在分词器中添加特殊token以处理图像和填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在分词器中添加特殊token\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<image>', '<unk>', '<pad>']})\n",
    "tokenizer.pad_token = '<pad>'\n",
    "tokenizer.unk_token = '<unk>'\n",
    "\n",
    "print(f\"Tokenizer vocabulary size after adding special tokens: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建 LLaVA 配置参数\n",
    "\n",
    "现在我们将创建一个结合视觉和语言模型配置的配置参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取视觉模型和语言模型的配置参数\n",
    "vision_config = vision_model.vision_model.config\n",
    "language_config = language_model.config\n",
    "\n",
    "# 创建一个结合视觉和语言模型配置的配置参数\n",
    "config = LlavaConfig(vision_config, language_config)\n",
    "\n",
    "# 在配置参数中设置图像token索引\n",
    "config.image_token_index = tokenizer.convert_tokens_to_ids('<image>')\n",
    "\n",
    "# 将聊天模板设置为原始 LLaVA 模板\n",
    "# 从 https://huggingface.co/llava-hf/llava-1.5-7b-hf/blob/main/chat_template.json 复制\n",
    "llava_template = \"\"\"\"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\\n' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\"\"\"\n",
    "\n",
    "# 创建一个结合图像处理器和分词器的处理器\n",
    "processor = LlavaProcessor(\n",
    "    image_processor=image_processor, \n",
    "    tokenizer=tokenizer,\n",
    "    patch_size=14,\n",
    "    chat_template=llava_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建 LLaVA 模型\n",
    "\n",
    "现在我们将通过结合视觉和语言模型来创建 LLaVA 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用结合的配置参数初始化 LLaVA 模型\n",
    "print(\"Building LLaVA model...\")\n",
    "model = LlavaForConditionalGeneration(config)\n",
    "\n",
    "# 分配预训练的语言模型\n",
    "model.language_model = language_model\n",
    "\n",
    "# 分配预训练的视觉模型\n",
    "model.vision_tower = vision_model\n",
    "\n",
    "# 调整令牌嵌入以匹配新的分词器大小\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 使模型参数连续以获得更好的性能\n",
    "make_model_contiguous(model)\n",
    "\n",
    "print(\"LLaVA model built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型\n",
    "\n",
    "最后，我们将保存组合模型和处理器到磁盘。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在保存模型之前，您可能需要指定 `CUDA_HOME` 环境变量以避免 CUDA 警告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_HOME\"] = \"/PATH/TO/YOUR/CUDA\" # run `which nvcc` to get the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型和处理器\n",
    "print(f\"Saving model to {save_path}...\")\n",
    "model.save_pretrained(save_path)\n",
    "processor.save_pretrained(save_path)\n",
    "print(\"Model and processor saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试模型\n",
    "\n",
    "让我们用一个示例图像测试我们新创建的 LLaVA 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What are these?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以先检查图像和提示词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们加载模型和处理器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载保存的模型和处理器\n",
    "loaded_model = LlavaForConditionalGeneration.from_pretrained(save_path)\n",
    "loaded_processor = LlavaProcessor.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们应该处理图像和提示词到 torch 张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = loaded_processor(images=raw_image, text=prompt, return_tensors='pt')\n",
    "print('The input has the following keys: ', inputs.keys())\n",
    "print('Their shapes are: ', {k: v.shape for k, v in inputs.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以生成回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = loaded_model.generate(**inputs, max_new_tokens=20, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如你所见，模型的输出非常混乱。这是因为模型没有在图像-文本对上进行训练。根据 LLaVA 论文中的介绍，我们应该在图像-文本对上微调模型以获得更好的性能。一个参考脚本在 `scripts/llava_step1.sh` 文件中提供。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模态扩展\n",
    "\n",
    "训练设置主要基于LLaVA论文。\n",
    "\n",
    "**注意**：您应该完成以下步骤以确保训练过程顺利进行。\n",
    "\n",
    "1. 通过以下命令下载`liuhaotian/LLaVA-Pretrain`数据集：\n",
    "\n",
    "```bash\n",
    "huggingface-cli download --repo-type dataset --resume-download liuhaotian/LLaVA-Pretrain --local-dir /PATH/TO/YOUR/liuhaotian/LLaVA-Pretrain --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "2. 解压`images.zip`文件，并将`images`文件夹放在`/PATH/TO/YOUR/images/`目录下。\n",
    "\n",
    "3. 将环境变量`COCO_DATA_DIR`指定为`images`文件夹的路径。\n",
    "\n",
    "最终脚本如下：\n",
    "\n",
    "\n",
    "```bash\n",
    "export COCO_DATA_DIR=\"/PATH/TO/YOUR/images/\"\n",
    "\n",
    "# Initialize variables\n",
    "MODEL_NAME_OR_PATH=\"/PATH/TO/YOUR/MLLMs\"\n",
    "TRAIN_DATASETS=\"/PATH/TO/YOUR/liuhaotian/LLaVA-Pretrain\"\n",
    "TRAIN_TEMPLATE=\"LLaVA_Pretrain\" # dataset template\n",
    "TRAIN_DATA_FILES=\"blip_laion_cc_sbu_558k.json\"\n",
    "\n",
    "OUTPUT_DIR=\"../outputs/llava_step1\" # output dir\n",
    "\n",
    "# For wandb online logging\n",
    "export WANDB_API_KEY=\"\"\n",
    "\n",
    "# Source the setup script\n",
    "source ./setup.sh\n",
    "\n",
    "# Execute deepspeed command\n",
    "deepspeed \\\n",
    "        --master_port ${MASTER_PORT} \\\n",
    "        --module align_anything.trainers.text_image_to_text.sft \\\n",
    "        --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n",
    "        --train_datasets ${TRAIN_DATASETS} \\\n",
    "        --train_template ${TRAIN_TEMPLATE} \\\n",
    "        --train_split train \\\n",
    "        --train_data_files ${TRAIN_DATA_FILES} \\\n",
    "        --output_dir ${OUTPUT_DIR} \\\n",
    "        --save_total_limit 3 \\\n",
    "        --freeze_vision_tower True \\\n",
    "        --freeze_mm_proj False \\\n",
    "        --freeze_language_model True \\\n",
    "        --epochs 1 \\\n",
    "        --ds_cfgs ds_z2_config.json \\\n",
    "        --learning_rate 1.e-3 \\\n",
    "        --per_device_train_batch_size 16 \\\n",
    "        --gradient_accumulation_steps 32\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "恭喜！您已成功通过结合语言模型和视觉模型构建了一个多模态LLaVA模型。该模型现在可以处理文本和图像，从而实现各种多模态应用。\n",
    "\n",
    "### 致谢\n",
    "\n",
    "- [Hugging Face Transformers 文档](https://huggingface.co/docs/transformers/index)\n",
    "- [LLaVA 论文](https://arxiv.org/abs/2304.08485)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
